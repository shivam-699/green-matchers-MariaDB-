from fastapi import FastAPI, HTTPException, Depends, WebSocket, WebSocketDisconnect, Request
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from fastapi.middleware.cors import CORSMiddleware
import numpy as np
from fastapi.responses import StreamingResponse
import requests
from deep_translator import GoogleTranslator
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
import jwt
from typing import Optional, List
from datetime import datetime, timedelta
from pydantic import BaseModel, validator
import logging
import time
import os
from dotenv import load_dotenv
import smtplib
from email.mime.text import MIMEText
import asyncio
from functools import lru_cache
import io
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from sentence_transformers import SentenceTransformer
from transformers import pipeline
from diffusers import StableDiffusionPipeline
from math import radians, sin, cos, sqrt, atan2
import mariadb
from sklearn.linear_model import LinearRegression  # Replaced TensorFlow with scikit-learn



# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment
load_dotenv()

# Global variables
app = FastAPI(title="Green Matchers API v3.3", version="3.3.0")

# Production Security - CORS + Rate Limit
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# MariaDB configuration
db_config = {
    'user': 'root',
    'password': os.getenv("DB_PASSWORD", "pass"),
    'host': 'localhost',
    'port': 3306,
    'database': 'green_jobs'
}

def get_db_connection():
    try:
        conn = mariadb.connect(**db_config)
        return conn
    except mariadb.Error as e:
        logger.error(f"Error connecting to MariaDB: {e}")
        return None

# JWT CONFIGURATION
SECRET_KEY = os.getenv("SECRET_KEY", "your-super-secure-secret-key-2025")
ALGORITHM = "HS256"
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

# Expanded Real Indian Green Companies mapped to skills
companies = {
    "python": ["Tata Power Renewables", "Adani Green Energy", "ReNew Power", "NTPC Renewable Energy", "Avaada Group", "Suzlon Energy", "Sterling and Wilson Renewable Energy", "Greenko Group", "Azure Power", "JSW Energy"],
    "design": ["Avaada Group", "Suzlon Energy", "Sterling and Wilson Renewable Energy", "Greenko Group", "Sova Solar", "Mytrah Energy", "Azure Power", "JSW Energy", "NTPC Renewable Energy", "ReNew Power"],
    "data": ["NTPC Renewable Energy", "Azure Power", "JSW Energy", "Mytrah Energy", "Greenko Group", "Avaada Group", "Suzlon Energy", "ReNew Power", "Adani Green Energy", "Tata Power Renewables"],
    "sustainable": ["Greenko Group", "Sova Solar", "Mytrah Energy", "Suzlon Energy", "Avaada Group", "Azure Power", "JSW Energy", "NTPC Renewable Energy", "ReNew Power", "Adani Green Energy"],
    "default": ["Tata Power Renewables", "Adani Green Energy", "ReNew Power", "NTPC Renewable Energy", "Avaada Group", "Suzlon Energy", "Sterling and Wilson Renewable Energy", "Greenko Group", "Azure Power", "JSW Energy"]
}

# Company Locations
company_locations = {
    "Tata Power Renewables": "Mumbai",
    "Adani Green Energy": "Ahmedabad",
    "ReNew Power": "Gurugram",
    "Suzlon Energy": "Pune",
    "Sterling and Wilson Renewable Energy": "Mumbai",
    "Azure Power": "New Delhi",
    "JSW Energy": "Mumbai",
    "Avaada Group": "Noida",
    "Greenko Group": "Hyderabad",
    "Sova Solar": "Kolkata",
    "Mytrah Energy": "Hyderabad",
    "NTPC Renewable Energy": "New Delhi"
}

# Company Reviews
company_reviews = {
    "Tata Power Renewables": {"rating": 4.0, "reviews": 442},
    "Adani Green Energy": {"rating": 3.7, "reviews": 120},
    "ReNew Power": {"rating": 3.8, "reviews": 95},
    "Suzlon Energy": {"rating": 3.9, "reviews": 150},
    "Sterling and Wilson Renewable Energy": {"rating": 4.0, "reviews": 200},
    "Azure Power": {"rating": 4.1, "reviews": 100},
    "JSW Energy": {"rating": 4.0, "reviews": 280},
    "Avaada Group": {"rating": 4.9, "reviews": 19},
    "Greenko Group": {"rating": 4.0, "reviews": 110},
    "Sova Solar": {"rating": 4.0, "reviews": 50},
    "Mytrah Energy": {"rating": 3.9, "reviews": 80},
    "NTPC Renewable Energy": {"rating": 3.8, "reviews": 250}
}

company_websites = {
    "Tata Power Renewables": "https://www.tatapower.com/careers",
    "Adani Green Energy": "https://www.adanigreenenergy.com/careers",
    "ReNew Power": "https://www.renewpower.in/careers",
    "Suzlon Energy": "https://www.suzlon.com/careers",
    "NTPC Renewable Energy": "https://www.ntpc.co.in/careers",
    "Avaada Group": "https://avaada.com/careers",
    "Greenko Group": "https://www.greenko.in/careers",
    "JSW Energy": "https://www.jsw.in/energy/careers",
    "Azure Power": "https://www.azurepower.com/careers",
    "Sterling and Wilson Renewable Energy": "https://www.sterlingandwilsonre.com/careers"
}

# Supported languages
SUPPORTED_LANGUAGES = {
    "en": "english", "hi": "hindi", "bn": "bengali", "mr": "marathi",
    "te": "telugu", "ta": "tamil", "gu": "gujarati", "ur": "urdu",
    "kn": "kannada", "or": "odia", "ml": "malayalam"
}

# Hindi Job Titles
HINDI_JOBS = {
    1: "‡§á‡§ï‡•ã ‡§á‡§Ç‡§ú‡•Ä‡§®‡§ø‡§Ø‡§∞",
    2: "‡§ó‡•ç‡§∞‡•Ä‡§® ‡§°‡•á‡§µ‡§≤‡§™‡§∞",
    3: "‡§®‡§µ‡•Ä‡§ï‡§∞‡§£‡•Ä‡§Ø ‡§µ‡§ø‡§∂‡•ç‡§≤‡•á‡§∑‡§ï",
    4: "‡§∏‡§∏‡•ç‡§ü‡•á‡§®‡•á‡§¨‡§ø‡§≤‡§ø‡§ü‡•Ä ‡§ï‡§Ç‡§∏‡§≤‡•ç‡§ü‡•á‡§Ç‡§ü",
    5: "‡§ó‡•ç‡§∞‡•Ä‡§® ‡§°‡•á‡§ü‡§æ ‡§∏‡§æ‡§á‡§Ç‡§ü‡§ø‡§∏‡•ç‡§ü"
}

# Global AI Models
model = None
generator = None
sd_pipe = None

# ENHANCED PYDANTIC MODELS
class SkillInput(BaseModel):
    skill_text: str
    lang: str = "en"

    @validator('skill_text')
    def validate_skill(cls, v):
        if len(v.strip()) < 2:
            raise ValueError('Skill must be at least 2 characters')
        return v

class JobInput(BaseModel):
    job_title: str
    job_description: str
    lang: str = "en"

class QueryInput(BaseModel):
    skill_text: List[str]
    lang: str = "en"
    location: Optional[str] = None

class ApplyInput(BaseModel):
    job_id: int
    cover_letter: str = ""

class CareerPathInput(BaseModel):
    current_skill: str
    years_experience: int = 5

class ImpactInput(BaseModel):
    role: str
    hours_per_week: int
    duration_months: int

def train_salary_predictor():
    # Simple linear regression instead of LSTM
    data = np.array([[8, 9], [6, 7], [7, 8], [10, 11]])
    X, y = data[:, 0:1], data[:, 1]
    model = LinearRegression()
    model.fit(X, y)
    return model

salary_model = train_salary_predictor()

# FIXED get_cached_jobs FUNCTION
@lru_cache(maxsize=128)
def translate_text_cached(text, lang):
    return GoogleTranslator(source='auto', target=lang).translate(text)

def get_cached_jobs(query: Optional[QueryInput] = None):
    conn = get_db_connection()
    if not conn:
        logger.error("Database connection failed")
        return []
    cursor = conn.cursor(dictionary=True)
    try:
        skill_text = " ".join(query.skill_text).lower() if query else ""
        query_params = []
        sql = """
            SELECT job_id AS id, title AS job_title, description, company, location, salary,
                   'SDG 7: 9/10 | Carbon Saved: 500 tons/year' AS sdg_impact,
                   '4.8‚≠ê' AS company_rating, 'High Demand' AS urgency
            FROM jobs WHERE 1=1
        """
        if query and query.location:
            sql += " AND location LIKE %s"
            query_params.append(f"%{query.location}%")
        if skill_text:
            sql += " AND (title LIKE %s OR description LIKE %s)"
            query_params.extend([f"%{skill_text}%", f"%{skill_text}%"])
        cursor.execute(sql, query_params)
        base_jobs = cursor.fetchall()
        matches = []
        skill_key = "default"
        if query:
            for skill in query.skill_text:
                for key in companies:
                    if key in skill.lower():
                        skill_key = key
                        break
        for job in base_jobs:
            company = companies[skill_key][job["id"] % len(companies[skill_key])]
            job_title = HINDI_JOBS.get(job["id"], job["job_title"]) if query and query.lang == "hi" else job["job_title"]
            description = translate_text_cached(job["description"], query.lang) if query and query.lang == "hi" else job["description"]
            matches.append({
                "id": job["id"],
                "job_title": job_title,
                "description": description,
                "salary": f"‚Çπ{job['salary']:.1f} LPA",
                "location": job["location"],
                "company": company,
                "company_rating": job["company_rating"],
                "sdg_impact": job["sdg_impact"],
                "urgency": job["urgency"],
                "website": company_websites.get(company),
                "similarity": 0.95
            })
        return matches
    except mariadb.Error as e:
        logger.error(f"Error querying jobs: {e}")
        return []
    finally:
        cursor.close()
        conn.close()

# Load AI Models
def load_models():
    global model, generator, sd_pipe
    print("üîÑ Loading AI Models...")
    model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')
    generator = pipeline("text-generation", model="gpt2", max_new_tokens=100, truncation=True)
    sd_pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4", safety_checker=None)
    sd_pipe = sd_pipe.to("cpu")
    print("‚úÖ AI Models Loaded!")

# Initialize Database
def init_db():
    conn = None
    cursor = None
    try:
        conn = mariadb.connect(**db_config)
        cursor = conn.cursor()
        # Create job_demand table with unique constraint
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS job_demand (
                demand_id INT AUTO_INCREMENT PRIMARY KEY,
                job_title VARCHAR(255),
                location VARCHAR(255) UNIQUE,
                demand_score INT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        # Create companies table
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS companies (
                company_id INT AUTO_INCREMENT PRIMARY KEY,
                name VARCHAR(255) UNIQUE,
                location VARCHAR(255),
                industry VARCHAR(100),
                size VARCHAR(50),
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        # Clear existing job_demand data
        cursor.execute("DELETE FROM job_demand")
        # Insert unique sample data
        sample_demand = [
            ('Green Jobs', 'Bengaluru', 15),
            ('Green Jobs', 'Mumbai', 10),
            ('Green Jobs', 'Delhi', 8),
            ('Green Jobs', 'Pune', 5),
            ('Green Jobs', 'Hyderabad', 7),
        ]
        cursor.executemany("INSERT IGNORE INTO job_demand (job_title, location, demand_score) VALUES (%s, %s, %s)", sample_demand)
        # Create other tables
        cursor.execute("CREATE TABLE IF NOT EXISTS users (user_id INT AUTO_INCREMENT PRIMARY KEY, username VARCHAR(255), password VARCHAR(255), email VARCHAR(255), role VARCHAR(50), created_at DATETIME)")
        cursor.execute("CREATE TABLE IF NOT EXISTS jobs (job_id INT AUTO_INCREMENT PRIMARY KEY, title VARCHAR(255), description TEXT, company VARCHAR(255), location VARCHAR(255), salary DECIMAL(10,2), posted_by INT, created_at DATETIME)")
        cursor.execute("CREATE TABLE IF NOT EXISTS favorites (user_id INT, job_id INT, PRIMARY KEY (user_id, job_id))")
        conn.commit()
        print("‚úÖ Database initialized with sample data")
    except mariadb.Error as e:
        print(f"Database Error: {e}")
    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()
    load_models()
    global salary_model
    salary_model = train_salary_predictor()
    return True



# Enhanced Auto-Geolocation using ipinfo.io
def get_city_from_ip(ip):
    try:
        api_key = os.getenv("IPINFO_API_KEY", "your-api-key-here")
        response = requests.get(f"https://ipinfo.io/{ip}/city?token={api_key}").text
        return response if response else "Unknown"
    except:
        return "Unknown"

# Distance Calculation
def calculate_distance(loc1, loc2):
    return 0 if loc1 == loc2 else 10

# WebSocket Manager
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            await connection.send_text(message)

manager = ConnectionManager()

def send_email(to_email: str, subject: str, body: str):
    print(f"üìß EMAIL SENT TO {to_email}: {subject}")
    print(f"üìß CONTENT: {body[:100]}...")
    return True

# AI Salary Prediction
def ai_salary_predictor(skill, years):
    base_salary = {"python": 8, "design": 6, "data": 7, "sustainable": 10}.get(skill.lower(), 8)
    return base_salary + years, base_salary + years + 5

def recommend_skills(skills):
    return ["Solar Panel Design", "Wind Energy Analysis"] if "python" in skills else ["Green Coding", "Sustainability"]

def generate_interview_questions(skills):
    return ["Tell me about your Python experience.", "How would you optimize renewable energy code?"]

def build_resume_pdf(username, skills):
    buffer = io.BytesIO()
    p = canvas.Canvas(buffer, pagesize=letter)
    p.drawString(100, 750, f"Resume - {username}")
    p.drawString(100, 730, f"Skills: {skills}")
    p.showPage()
    p.save()
    buffer.seek(0)
    return buffer.getvalue()

# PRODUCTION ENDPOINTS v3.3
@app.get("/health")
def health_check():
    return {"status": "healthy", "version": "3.3.0", "features": ["Auto-Geo", "Distance", "Salary Boost", "Interview", "Resume", "Trends", "Cover Letter"]}


@app.get("/stats")
def get_stats():
    conn = get_db_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")
    cursor = conn.cursor()
    try:
        # Get real counts from database
        cursor.execute("SELECT COUNT(*) FROM users")
        users_count = cursor.fetchone()[0]
        cursor.execute("SELECT COUNT(*) FROM jobs")
        jobs_count = cursor.fetchone()[0]
        cursor.execute("SELECT COUNT(*) FROM favorites")
        favorites_count = cursor.fetchone()[0]
        cursor.execute("SELECT COUNT(*) FROM companies")
        companies_count = cursor.fetchone()[0]
        
        # Return stats matching frontend expectations
        return {
            "total_jobs": 547,           # Realistic market number
            "companies": companies_count, # Actual companies count from DB
            "sdg_goals": 15,             # Expanded SDG coverage
            "favorites": favorites_count, # Real favorites count from DB
            "applications": 8,           # User applications (mock for now)
            "profile_views": 143         # User profile views (mock for now)
        }
    except mariadb.Error as e:
        logger.error(f"Error querying stats: {e}")
        raise HTTPException(status_code=500, detail="Database query failed")
    finally:
        cursor.close()
        conn.close()



@app.get("/job_trends")
async def job_trends():
    try:
        conn = mariadb.connect(**db_config)
        cursor = conn.cursor()
        cursor.execute("SELECT location, demand_score FROM job_demand GROUP BY location")
        rows = cursor.fetchall()
        labels = [row[0] for row in rows]
        data = [row[1] for row in rows]
        cursor.close()
        conn.close()
        return {
            "chart": {
                "type": "bar",
                "data": {
                    "labels": labels,
                    "datasets": [{
                        "label": "Job Demand",
                        "data": data,
                        "backgroundColor": ["#FF6384", "#36A2EB", "#FFCE56", "#4BC0C0", "#9966FF"]
                    }]
                },
                "options": {"scales": {"y": {"beginAtZero": True}}}
            }
        }
    except mariadb.Error as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")

@app.get("/dashboard")
async def dashboard():
    predictions = salary_model.predict(np.array([[10]]))[0]  # Updated for scikit-learn
    chart_data = [8, 9, 7, 11, float(predictions)]
    return {"chart": {"type": "line", "data": {"labels": ["Jan", "Feb", "Mar", "Apr", "Future"], "datasets": [{"data": chart_data, "backgroundColor": "#36A2EB"}]}}}

@app.post("/token")
async def login(form_data: OAuth2PasswordRequestForm = Depends()):
    conn = get_db_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")
    cursor = conn.cursor(dictionary=True)
    try:
        cursor.execute("SELECT username, password, role, email FROM users WHERE username = %s", (form_data.username,))
        user = cursor.fetchone()
        if not user or user["password"] != form_data.password:  # Plaintext for simplicity; hash in production
            raise HTTPException(status_code=401, detail="Invalid credentials")
        access_token = create_access_token(data={"sub": user["username"]})
        return {"access_token": access_token, "token_type": "bearer", "user": user["username"]}
    except mariadb.Error as e:
        logger.error(f"Error during login: {e}")
        raise HTTPException(status_code=500, detail="Database query failed")
    finally:
        cursor.close()
        conn.close()

# JWT + AUTH FUNCTIONS
def create_access_token(data: dict, expires_delta: timedelta = None):
    to_encode = data.copy()
    expire = datetime.utcnow() + (timedelta(minutes=15) if not expires_delta else expires_delta)
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

def get_current_user(token: str = Depends(oauth2_scheme)):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        conn = get_db_connection()
        if not conn:
            raise HTTPException(status_code=500, detail="Database connection failed")
        cursor = conn.cursor(dictionary=True)
        try:
            cursor.execute("SELECT username, role, email FROM users WHERE username = %s", (username,))
            user = cursor.fetchone()
            if not user:
                raise HTTPException(status_code=401, detail="Invalid credentials")
            return user
        except mariadb.Error as e:
            logger.error(f"Error verifying user: {e}")
            raise HTTPException(status_code=500, detail="Database query failed")
        finally:
            cursor.close()
            conn.close()
    except jwt.ExpiredSignatureError:
        raise HTTPException(status_code=401, detail="Token expired")
    except jwt.InvalidTokenError:
        raise HTTPException(status_code=401, detail="Invalid token")

@app.post("/match_jobs")
@limiter.limit("10/minute")
async def match_jobs(request: Request, query: QueryInput, current_user: dict = Depends(get_current_user)):
    start_time = time.time()
    user_city = get_city_from_ip(request.client.host)
    if not query.location or query.location.lower() == "string":
        query.location = user_city
        print(f"üë§ AUTO-DETECTED: {user_city}")
    jobs = get_cached_jobs(query)
    skill_text = " ".join(query.skill_text).lower()
    matches = []
    for job in jobs:
        similarity = 0.95 if "python" in skill_text else 0.90
        if query.location:
            job_location = job["location"].lower()
            user_location = query.location.lower()
            if user_location in job_location or job_location in user_location:
                similarity += 0.05
                distance = calculate_distance(user_location, job_location)
            else:
                continue
        salary_min, salary_max = ai_salary_predictor(skill_text, 5)
        salary_boost = f"‚Çπ{salary_min}-{salary_max} LPA (+12%)"
        matches.append({
            "id": job["id"], "job_title": job["job_title"], "description": job["description"],
            "salary_range": job["salary"], "salary_boost": salary_boost,
            "location": job["location"], "distance_km": distance,
            "company": job["company"], "website": job["website"],
            "company_rating": job["company_rating"], "sdg_impact": job["sdg_impact"],
            "urgency": job["urgency"], "similarity": round(similarity, 2),
            "apply_url": f"https://greenmatchers.com/jobs/{job['id']}"
        })
    matches = sorted(matches, key=lambda x: x["similarity"], reverse=True)
    response_time = time.time() - start_time
    await manager.broadcast(f"üö® {current_user['username']}: {len(matches)} JOBS in {query.location}!")
    send_email(current_user["email"], "üö® NEW GREEN JOBS!", f"{len(matches)} matches in {query.location}!")
    return {
        "matches": matches[:5],
        "user_location": query.location,
        "auto_detected": user_city == query.location,
        "suggestions": recommend_skills(skill_text)[:2],
        "response_time": f"{response_time:.2f}s",
        "total_jobs": len(matches),
        "user": current_user["username"]
    }

@app.post("/generate_interview_prep")
async def interview_prep(query: QueryInput, current_user: dict = Depends(get_current_user)):
    questions = generate_interview_questions(" ".join(query.skill_text))
    return {"interview_questions": questions, "company": "Tata Power Renewables", "user": current_user["username"]}

@app.post("/generate_resume")
async def generate_resume(query: QueryInput, current_user: dict = Depends(get_current_user)):
    pdf_bytes = build_resume_pdf(current_user["username"], " ".join(query.skill_text))
    return StreamingResponse(
        io.BytesIO(pdf_bytes),
        media_type="application/pdf",
        headers={"Content-Disposition": f"attachment; filename=resume_{current_user['username']}.pdf"}
    )

@app.post("/save_job")
async def save_job(job_id: int, current_user: dict = Depends(get_current_user)):
    conn = get_db_connection()
    if not conn:
        raise HTTPException(status_code=500, detail="Database connection failed")
    cursor = conn.cursor()
    try:
        cursor.execute("SELECT user_id FROM users WHERE username = %s", (current_user["username"],))
        user_id = cursor.fetchone()[0]
        cursor.execute("INSERT IGNORE INTO favorites (user_id, job_id) VALUES (%s, %s)", (user_id, job_id))
        conn.commit()
        cursor.execute("SELECT COUNT(*) FROM favorites WHERE user_id = %s", (user_id,))
        favorites_count = cursor.fetchone()[0]
        return {"message": "Job saved!", "favorites": favorites_count}
    except mariadb.Error as e:
        conn.rollback()
        logger.error(f"Error saving job: {e}")
        raise HTTPException(status_code=400, detail=str(e))
    finally:
        cursor.close()
        conn.close()

@app.post("/career_path")
async def career_path(career_data: CareerPathInput, current_user: dict = Depends(get_current_user)):
    path = {
        "python": ["Junior Eco Engineer", "Senior Green Developer", "CTO Sustainability"],
        "design": ["Junior Designer", "Lead Architect", "Head of Green Design"],
        "data": ["Junior Analyst", "Senior Data Scientist", "Chief Climate Officer"]
    }.get(career_data.current_skill.lower(), ["Green Specialist", "Senior Expert", "Director"])
    salary_min, salary_max = ai_salary_predictor(career_data.current_skill, career_data.years_experience)
    return {
        "current_skill": career_data.current_skill,
        "years": career_data.years_experience,
        "career_path": path,
        "salary_projection": f"‚Çπ{salary_min}-{salary_max} LPA",
        "company": "Tata Power Renewables",
        "sdg_impact": "Maximum contribution to 7 SDGs"
    }

@app.post("/generate_cover_letter")
async def generate_cover_letter(query: QueryInput, current_user: dict = Depends(get_current_user)):
    cover_letter = f"Dear Hiring Manager,\nI am excited to apply for the {query.skill_text[0]} role at {next(iter(company_websites))}. With my experience in {query.skill_text[0]}, I can contribute to your green initiatives.\nBest,\n{current_user['username']}"
    return {"cover_letter": cover_letter, "user": current_user["username"]}

@app.post("/simulate_impact")
async def simulate_impact(input: ImpactInput, current_user: dict = Depends(get_current_user)):
    impact_per_hour = {"Eco Engineer": 0.5, "Green Developer": 0.3, "Renewable Analyst": 0.4}
    total_impact = (impact_per_hour.get(input.role, 0.1) * input.hours_per_week * (input.duration_months * 4)) / 1000
    return {
        "chart": {
            "type": "pie",
            "data": {
                "labels": ["CO2 Saved", "Remaining Impact"],
                "datasets": [{
                    "data": [total_impact, 1 - total_impact],
                    "backgroundColor": ["#4BC0C0", "#FFCE56"]
                }]
            }
        },
        "total_impact_tons": f"{total_impact:.2f} tons",
        "user": current_user["username"]
    }

@app.get("/trends/skills")
async def get_skills_trends():
    try:
        # For hackathon demo - return realistic skills data
        return {
            "skills": [
                {"name": "Solar PV Design", "demand": 95, "growth": "+25%", "jobs": 145},
                {"name": "Carbon Accounting", "demand": 92, "growth": "+30%", "jobs": 128},
                {"name": "ESG Reporting", "demand": 88, "growth": "+22%", "jobs": 156},
                {"name": "Renewable Analytics", "demand": 85, "growth": "+20%", "jobs": 112},
                {"name": "Green Building (LEED)", "demand": 82, "growth": "+18%", "jobs": 98},
                {"name": "Wind Energy Systems", "demand": 78, "growth": "+15%", "jobs": 87}
            ]
        }
    except Exception as e:
        logger.error(f"Error fetching skills trends: {e}")
        return {"skills": []}

@app.get("/trends/companies")
async def get_companies_trends():
    return {
        "companies": [
            {"name": "Tata Power Renewables", "openings": 47, "growth": "+35%", "rating": 4.5},
            {"name": "Adani Green Energy", "openings": 38, "growth": "+28%", "rating": 4.3},
            {"name": "ReNew Power", "openings": 32, "growth": "+22%", "rating": 4.4},
            {"name": "Suzlon Energy", "openings": 28, "growth": "+18%", "rating": 4.2},
            {"name": "Azure Power", "openings": 24, "growth": "+30%", "rating": 4.3},
            {"name": "Hero Future Energies", "openings": 19, "growth": "+25%", "rating": 4.1}
        ]
    }

# Real-time WebSocket connections
active_connections = []

# Real-time WebSocket connections
active_connections = []

@app.websocket("/ws/stats")
async def websocket_stats(websocket: WebSocket):
    await websocket.accept()
    print("‚úÖ WebSocket client connected")
    active_connections.append(websocket)
    
    try:
        while True:
            # Send updates every 10 seconds for demo
            await asyncio.sleep(10)
            
            try:
                # Get live stats from database
                conn = get_db_connection()
                if conn:
                    cursor = conn.cursor()
                    try:
                        cursor.execute("SELECT COUNT(*) FROM jobs")
                        total_jobs_result = cursor.fetchone()
                        total_jobs = total_jobs_result[0] if total_jobs_result else 547
                        
                        cursor.execute("SELECT COUNT(DISTINCT company) FROM jobs")
                        companies_result = cursor.fetchone()
                        companies = companies_result[0] if companies_result else 51
                        
                        await websocket.send_json({
                            "type": "stats_update",
                            "total_jobs": total_jobs,
                            "companies": companies,
                            "timestamp": datetime.now().strftime("%H:%M:%S")
                        })
                        print(f"üìä Sent stats update: {total_jobs} jobs, {companies} companies")
                        
                    except Exception as db_error:
                        print(f"Database error: {db_error}")
                        # Fallback data if DB query fails
                        await websocket.send_json({
                            "type": "stats_update", 
                            "total_jobs": 547,
                            "companies": 51,
                            "timestamp": datetime.now().strftime("%H:%M:%S")
                        })
                    finally:
                        cursor.close()
                        conn.close()
                else:
                    # Fallback if DB connection fails
                    await websocket.send_json({
                        "type": "stats_update",
                        "total_jobs": 547,
                        "companies": 51, 
                        "timestamp": datetime.now().strftime("%H:%M:%S")
                    })
                    
            except Exception as send_error:
                print(f"Error sending WebSocket message: {send_error}")
                break
                
    except WebSocketDisconnect:
        print("‚ùå WebSocket client disconnected")
        active_connections.remove(websocket)
    except Exception as e:
        print(f"‚ùå WebSocket error: {e}")
        active_connections.remove(websocket)
    finally:
        if websocket in active_connections:
            active_connections.remove(websocket)
        print("üîå WebSocket connection cleaned up")
# Initialize
init_db()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)